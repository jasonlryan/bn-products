# AI Innovation Programme • Discovery Qualification

Below is a practical, salesperson-ready discovery & qualification framework tailored to Brilliant Noise’s AI Innovation Programme. Use this in discovery calls, qualification checks, and handoffs to proposals/engagement teams.

1) 10 discovery questions (mapped to BANT + MEDDIC), with phrasing, what to listen for (ideal answers) and red-flag answers
- Format: Question — (BANT / MEDDIC) — How to ask it — Ideal answer (evidence) — Red-flag answer

1. What business outcome do you need to deliver from AI innovation in the next 6–12 months?  
   - (Need / Identify Pain + Metrics)  
   - How to ask: “What specific business outcomes are you trying to accelerate with AI this year?”  
   - Listen for: concrete KPIs (time-to-market reduction, revenue/ARR uplift, cost savings, number of features shipped per year, patents). Example: “We need to ship 3 AI features this year and cut dev cycle by 4x.”  
   - Red flag: vague answers (“we want to explore AI”) or low-priority initiatives.

2. What budget or R&D/innovation funding is available for pilots or programme setup? (range)  
   - (Budget / Metrics + Economic Buyer)  
   - How to ask: “Do you have a dedicated innovation or R&D budget for pilots? What range could you allocate to a 90-day innovation sprint?”  
   - Listen for: named budget lines, approval thresholds, confirm ability to fund from ~£25k upward. Evidence: budget owner, year-to-date spend.  
   - Red flag: “No budget,” only discretionary spend later in year, or only maintenance budgets.

3. Who is the decision-maker for innovation vendor selection and who signs off budget?  
   - (Authority / Economic Buyer)  
   - How to ask: “Who will approve this engagement and who signs the PO/contract?”  
   - Listen for: named exec(s) (CMO, CDO, Head of R&D) and procurement involvement. Evidence: org chart or list of approvers.  
   - Red flag: procurement-only contact with no exec sponsor.

4. What is your timeline to pilot and to scale a winning prototype?  
   - (Timing / Decision Process + Metrics)  
   - How to ask: “When do you need a validated prototype and when would you commit to scaling winners?”  
   - Listen for: 90-day prototype expectation, roadmap integration windows, quarterly planning cycles.  
   - Red flag: “No timeline,” multi-year horizon, or procurement cycles >6 months.

5. What decision criteria will you use to evaluate success (e.g., ROI, IP, speed, integration cost)?  
   - (Need/Authority / Decision Criteria)  
   - How to ask: “What outcomes will determine whether a pilot is a success? What criteria will you use to choose a partner?”  
   - Listen for: desire for measurable ROI, patentability, integration feasibility, stakeholder buy-in.  
   - Red flag: criteria solely price-based or undefined success metrics.

6. What existing AI/R&D capability, data access and engineering capacity do you have?  
   - (Need / Identify Pain + Champion)  
   - How to ask: “Tell me about the teams, tech stack and data access we’d integrate with—ML teams, cloud, MLOps, product engineering?”  
   - Listen for: named teams, data access policies, cloud providers, CI/CD and analysts available for collaboration.  
   - Red flag: no internal engineers allocated, data locked down with no access, legacy-only stack.

7. Tell me about recent AI or innovation experiments—what worked, what stalled, and why?  
   - (Need / Metrics + Identify Pain)  
   - How to ask: “What experiments have you done lately? Which scaled and which didn’t—and what stopped them?”  
   - Listen for: specific failure modes (no decision path, lack of funding, integration pain) and appetite to change.  
   - Red flag: experiments never progressed beyond PoC and no interest in process change.

8. Who would act as the internal champion for this programme and what capacity will they dedicate?  
   - (Authority / Champion)  
   - How to ask: “Who inside will own the project day-to-day and how many FTE hours can they commit?”  
   - Listen for: product/innovation lead committed to sprints, named stakeholders, time allocation (e.g., 1–2 FTEs for 90 days).  
   - Red flag: “We’ll appoint someone later” or only part-time attention from busy stakeholders.

9. Are there legal, compliance, IP or procurement constraints that could block rapid prototyping or IP ownership?  
   - (Timing / Decision Process + Decision Criteria)  
   - How to ask: “Any constraints on data use, vendor IP, or compliance sign-offs we should know about?”  
   - Listen for: known issues but workable processes, IP strategy open to co-developed patents, standard NDAs possible.  
   - Red flag: blanket prohibition on external prototypes, IP cannot be shared or patented, protracted legal approval.

10. If a prototype meets success criteria in 90 days, what’s your likely path to scale and funding?  
   - (Timing/Budget / Metrics + Decision Process)  
   - How to ask: “Assuming success, how would you move from prototype to production—what approvals, funding and teams are required?”  
   - Listen for: clear scale path (product roadmap slot, budget to build-out, exec sign-off).  
   - Red flag: no path to scale, no follow-on budget, or no roadmap integration plan.

2) Red-flag indicators for disqualification (actionable)
- No committed budget (or budget restricted to non-innovation/maintenance) and no clear budget cycle to access funds within 6–12 months.
- No executive sponsor / economic buyer unwilling to engage (procurement-only contact).
- No timeline or horizon beyond 12–18 months; procurement cycles >6 months with no fast-track option.
- No internal capacity or unwilling to allocate product/engineering time to collaborate during a 90-day sprint.
- Data, compliance or IP rules that legally prevent external prototyping or prevent IP ownership/patenting.
- Decision criterion is purely lowest-cost procurement or vendor lock-in preference (we’re not a low-cost commodity).
- Organization is highly risk-averse, with “pilot fatigue” (many failed PoCs with no appetite for repeatable process change).
- Tech stack is non-integrable (legacy systems with no API/no data access) and no migration plan is available.
- No measurable KPIs; inability or unwillingness to define success metrics.
- Industry-specific regulatory blockers (e.g., certain financial institutions with unacceptable constraints) when legal cannot be aligned.

If multiple red flags appear, recommend disqualification or a long-term nurture plan rather than active pursuit.

3) Ideal customer scoring criteria (1–10 scale) — how to score during/after discovery
Score each of five dimensions 1–10. Weighted average -> total score (convert to 1–10). Use this to prioritise pipeline.

Scoring dimensions and guidance:
- Strategic Fit (weight 30%) — How aligned is the engagement to our value prop (quarterly pipeline, patentable innovation, product features)?  
  - 9–10: Needs quarterly innovation, aims for patent/IP, roadmap depends on AI.  
  - 5–8: Interested in faster product delivery but not necessarily IP.  
  - 1–4: Exploration only, not productised.

- Budget Readiness (weight 25%) — Availability of £25k+ or R&D budget and funding path for pilots and scaling.  
  - 9–10: Budget committed or clear approval path for £25–250k.  
  - 5–8: Budget possible but requires business case.  
  - 1–4: No budget or only maintenance funds.

- Decision Clarity & Authority (weight 20%) — Named economic buyer, decision timeline, procurement involvement.  
  - 9–10: Exec sponsor named and accessible; approval <8 weeks.  
  - 5–8: Multiple stakeholders, procurement involved; approval 8–12 weeks.  
  - 1–4: No sponsor, procurement-only, opaque process.

- Technical & Data Readiness (weight 15%) — Access to data, engineering capacity, cloud/MLOps readiness to support a 90-day prototype.  
  - 9–10: Engineers/data scientists available, data accessible via APIs, cloud infra in place.  
  - 5–8: Some access but may need work.  
  - 1–4: Data locked, no engineers, legacy stack.

- Cultural/Champion Strength (weight 10%) — Appetite for experimentation; internal champion actively pushing change.  
  - 9–10: Strong champion, culture of experimentation, prior scaled pilots.  
  - 5–8: Some interest but limited track record.  
  - 1–4: Resistant to change, pilot fatigue.

How to compute total score:
- Multiply each score by its weight, sum, then rescale to 1–10. Example quick method:
  - Total = (Strategic*0.3 + Budget*0.25 + Decision*0.2 + Tech*0.15 + Culture*0.1)
  - Total is already on 1–10 scale. Use that number for next-step decisioning.

Example interpretation:
- ≥8.5 — Ideal lead (high priority).  
- 6.5–8.4 — Good opportunity (proceed with tailored proposal).  
- 4.0–6.4 — Moderate (nurture; smaller pilots or proof-of-value).  
- <4.0 — Disqualify / long-term nurture list.

4) Next steps and actions based on qualification score (actionable roadmap)

Score ≥ 8.5 — Close/Immediate Pursuit (high probability)
- Next steps (within 7 days):
  - Schedule Executive Alignment Workshop (45–60 mins) with economic buyer, champion, Head of R&D/product. Agenda: confirm KPIs, decision criteria, timeline, scaling path and budget.  
  - Send short Pilot Brief template and SOW outline (90-day sprint, deliverables, success criteria, pricing from £25k).  
  - Legal/Procurement: issue NDA and standard T&Cs pre-approved.  
  - Technical prep: request sample datasets, system access scope, and list of engineers for onboarding.  
  - Goal: sign SOW or PO within 2–4 weeks, commence Test‑Learn‑Lead™ sprint.

Score 6.5–8.4 — Engage with a tailored proposal (mid-priority)
- Next steps (2–4 weeks):
  - Book a detailed scoping call with champion and product/engineering reps to flesh pilot scope and integration needs.  
  - Offer a scaled “Pilot Lite” option (e.g., discovery + 60-day prototype, reduced price) if budget approvals are uncertain.  
  - Deliver a proposal: timeline, outcomes, KPIs, resource requirements, and phased SOW (pilot → scale).  
  - Provide case studies focused on similar industry use-cases (e.g., patents, speed-to-market).  
  - If procurement involved, provide procurement-friendly documentation.  
  - Goal: convert to ≥£25k pilot or secure written budget commitment.

Score 4.0–6.4 — Nurture, validate capability & build sponsor (low–mid priority)
- Next steps (4–8 weeks):
  - Educate and build momentum: invite to a public or private Innovation Showcase (client examples, demo of 90-day outputs).  
  - Propose a very small Discovery Sprint (paid discovery at lower cost, or workshop at fixed fee) to create a one-page business case.  
  - Equip champion: send tailored ROI models and IP-focused case studies; offer reference calls with similar clients.  
  - Align calendar: identify next budget cycle and procurement windows; schedule a check-in in 6–12 weeks.  
  - Goal: convert to mid-level POC when budget/timeline align.

Score <4.0 — Disqualify or reclassify to long-term nurture
- Next steps:
  - Disqualify from active pipeline; move to nurture with cadence (quarterly).  
  - Provide a helpful resource pack (top FAQs, outcomes, small “how to get ready” checklist).  
  - If appropriate, refer to low-cost alternatives or partners (where you won’t compete).  
  - Re-assess in 6–12 months or when they hit specific triggers (new R&D head, budget allocation, regulatory changes).

5) Practical tips & artifacts to accelerate qualification and close
- During calls, always ask for tangible evidence: budget owner, budget line, org chart, timelines, and access to a sample dataset. These accelerate legal and technical onboarding.  
- Use simple templates we can reuse: Pilot Brief, 90-day SOW template, ROI calculator, legal NDA and vendor questionnaire. Keep procurement packs ready.  
- For Executive Workshop agenda (30–60 mins): Business outcomes, 90-day sprint scope, success criteria & KPIs, resource & data requirements, commercial model & next steps. Send pre-read the day before.  
- If budget is unclear, offer a “Pilot Lite” at a lower entry price with clearly-scoped outcomes to prove value quickly and unlock larger funding.  
- Always document decision criteria and sign-off process in writing. If the economic buyer is not on calls, request a 15-minute alignment with them before proposal.

6) Example red-flag remediation playbook (if you want to rescue borderline leads)
- No budget but strong strategic fit: propose flexible commercial models (phased payments, success fee, joint funding with R&D).  
- No champion: recruit a secondary sponsor (Head of Product) by demonstrating quick wins & executive briefing packs.  
- Data access issues: propose a synthetic-data or sandboxed prototype approach to validate concepts without production data.  
- Procurement delay: provide contract templates, security and SOC/ISO artifacts, and a procurement “one-pager” to speed reviews.

Use this framework on every early conversation. Score leads consistently, include evidence notes, and enforce the next-step timeline. That creates predictable pipeline hygiene and allows Brilliant Noise to focus on organisations ready to convert R&D budgets into a quarterly pipeline of patent-worthy AI outcomes.
