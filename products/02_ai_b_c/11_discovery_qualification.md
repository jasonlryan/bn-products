# AI-B-C™ • Discovery Qualification

Framework: AI-B-C™ Sales Discovery & Qualification
Purpose: Rapidly determine fit and likelihood of success for AI-B-C™ with CMOs/CDOs/Innovation execs at medium→large enterprises. Deliverable: 10 high-impact discovery questions (mapped to BANT + MEDDIC), clear red flags for disqualification, an ideal-customer scoring model (1–10), and prescriptive next steps by score.

1) Ten discovery questions — organised by BANT / MEDDIC (each question: mapping, why it matters, what a strong answer looks like, what’s a red-flag answer)
Note: Use these in a 30–45 minute executive discovery. Invite the likely economic buyer + potential champion; ask for an org chart & top business metrics as pre-read.

A. Budget (BANT) / Metrics (MEDDIC)
1. “What would success look like if your teams were using AI daily in 30–90 days? Which KPIs would you expect to move, and by how much?”
- Maps to: Budget (willingness to invest), Metrics (value)
- Why: Ties product outputs (90% weekly use, 20+ hrs saved) to their financial/operational goals.
- Strong answer: Specific KPIs (hours saved, cost per campaign, time-to-market, churn/retention uplift) with target magnitudes and ownership.
- Red flag: Vague “we want to be more innovative” with no measurable metrics.

B. Authority (BANT) / Economic Buyer (MEDDIC)
2. “Who’s the executive who must sign off on learning / people development spends, and who will champion this internally?”
- Maps to: Authority, Economic Buyer, Champion
- Why: Identifies decision-maker and internal sponsor.
- Strong answer: Names, timeline for approval, champion in marketing/innovation with authority to allocate budgets.
- Red flag: No named decision-maker or “we’ll need to ask procurement” with no internal sponsor.

C. Need / Pain (BANT & MEDDIC – Identify Pain)
3. “What’s the biggest productivity or capability gap across your marketing/product/innovation teams that you think AI could fix?”
- Maps to: Need/Pain, Identify Pain
- Why: Validates urgency & alignment with AI-B-C’s outcome-driven promise.
- Strong answer: Explicit pain like campaign bottlenecks, agency reliance, long content cycles, measurable wasted hours.
- Red flag: No clear operational pain or belief that existing processes are “good enough.”

D. Timing (BANT) / Decision Process (MEDDIC)
4. “When do you need measurable change in place—and what’s your decision process/timeline for vendor selection?”
- Maps to: Timing, Decision Process
- Why: Confirms whether they can meet our 30–90 day adoption window and procurement duration.
- Strong answer: 30–90 day expectation, clear decision steps, a target decision date within 30–60 days.
- Red flag: Procurement timeline >6 months or indecisive process.

E. Decision Criteria (MEDDIC)
5. “What criteria will you use to decide between training/enablement partners (e.g., case studies, metrics, security, value for money)?”
- Maps to: Decision Criteria
- Why: Reveals must-haves (security, vendor pedigree, ROI proof).
- Strong answer: Prioritises demonstrable ROI, role-specific outcomes, references from CPG/consumer brands.
- Red flag: Criteria are unrealistic (lowest price only) or dominated by internal politics.

F. Competition / Existing Tools (MEDDIC)
6. “What AI tools, learning platforms or pilots are already in place? If adoption stalled, why?”
- Maps to: Competition, Technical Fit
- Why: Identifies fragmentation and where AI-B-C™ can complement or replace efforts.
- Strong answer: Some pilots but no role-specific training or outcome measurement; appetite to consolidate.
- Red flag: Heavy enterprise contracts preventing new training or legal bans on third-party AI tools.

G. Economic Value / ROI (MEDDIC – Metrics)
7. “Do you have an expected ROI or cost-savings target for a programme like this? How does training & productivity factor into your P&L?”
- Maps to: Metrics / Economic Buyer
- Why: Confirms financial orientation and ease of business case creation.
- Strong answer: Has a target (e.g., reduce agency spend by X, productivity savings equating to Y £/month).
- Red flag: No interest in quantifying ROI or inability to see productivity as value.

H. Champion & Adoption (MEDDIC)
8. “Who inside the team would be the day-to-day lead/champion for adoption and progress tracking?”
- Maps to: Champion, Identify Pain
- Why: Adoption depends on an operational champion for roll-out & measurement.
- Strong answer: Named training lead / L&D partner / marketing ops manager ready to own progress tracking.
- Red flag: No internal resource or expectation that vendor will do everything with no client involvement.

I. Legal / Data / IT (Technical Fit)
9. “Are there regulatory, legal or IT constraints (data residency, IP, approved vendors) that could block hands-on AI training?”
- Maps to: Technical / Compliance risk
- Why: Hands-on workshops require access to tools/data and clearances.
- Strong answer: Minor constraints; willing to run redacted or sandboxed sessions and sign an NDA.
- Red flag: Blanket ban on external AI tools, no willingness to allow role-based sandboxing.

J. Budget & Funding Source (BANT)
10. “Do you have an allocated training/capability budget or will this be funded from transformation/innovation/marketing budgets?”
- Maps to: Budget
- Why: Confirms where money comes from and whether price points (£2k/£8.8k/£17.5k) are feasible.
- Strong answer: Budget line exists in L&D/marketing/innovation or clear approval route for these amounts.
- Red flag: No budget and no sponsorship to free funds this quarter/next quarter.

2) Red-flag indicators for disqualification (fast disqualifiers)
- Company size <50 employees (product targets 50+; ideal 150+).  
- No named economic buyer / no internal champion after two outreach attempts.  
- Procurement or legal procurement timeline > 6 months for small/medium programmes.  
- No training/development budget and no plan to reallocate funds.  
- Legal/IT prohibits any external AI tooling or hands-on sessions (cannot sandbox).  
- No measurable business metrics or refusal to quantify expected outcomes.  
- Stakeholder belief that AI is “nice to have” with no urgency; no operational pain.  
- Executive energy focused elsewhere for 12+ months (e.g., M&A, major restructure).  
- Competing enterprise contracts that lock training/agency spend and bar new vendors.  
- Repeated inability to produce the list of teams/roles that would attend within 3 contact cycles.

3) Ideal Customer Scoring criteria (1–10 per criterion) — what to score and how
Score each criterion 1–10 (1 = very weak / disqualifying, 10 = ideal). Sum or weight (see weighting guidance) to create a normalized score out of 100, or average to map to a 1–10 overall qualification score.

Recommended scoring dimensions (with weighting suggestions for total score; weights sum = 100%):
- Strategic Fit / Industry & Size (15%): Company 150+ employees, consumer/global brand = high score.
- Urgency / Pain (20%): Clear productivity or capability gap tied to KPIs.
- Budget Availability (15%): Existing training budget or approved spend.
- Economic Buyer / Authority (10%): Named decision-maker and alignment.
- Executive Sponsorship (10%): Exec alignment with 90-day adoption goal.
- Champion & Adoption Capacity (10%): Named operational champion ready to lead roll-out.
- Timeline / Decision Process (8%): Decision within 30–60 days; procurement straightforward.
- Technical / Legal Fit (7%): Sandbox, acceptable vendor security/compliance posture.
- ROI Clarity / Metrics (5%): Specific KPIs and measurable targets.

Scoring guidance examples (per criterion):
- 9–10: Ideal (e.g., 5000-50k employees, named CMO champion, budget approved, decision in 30 days)
- 6–8: Good (e.g., 150–1000 employees, budget likely, decision in 60 days)
- 3–5: Marginal (e.g., 50–149 employees, budget unsure, procurement may delay)
- 1–2: Poor (e.g., <50 employees, no budget, procurement >6 months, legal block)

Example calculation (quick method):
- Rate each criterion 1–10 → multiply by weight → sum → normalized to a 0–100 score.
- Map to 1–10 by dividing by 10 or use direct ranges (below).

4) Interpret scores & recommended next steps (actionable playbook)

A. Score 85–100 (Overall 9–10 / “Champion Deal — Close & Scale”)
- Diagnosis: Excellent fit, budget and buyer aligned, timeline tight, champion in place.
- Immediate actions (within 48–72 hrs):
  - Send tailored commercial proposal: Offer Complete Sprint (£17,500) with a 30–60 day start window.
  - Book Executive Briefing within 7–14 days (paid engagement) as a contractual kickoff and discovery step.
  - Prepare a short bespoke one-page ROI case using their KPIs (use pre-call data).
  - Identify implementation squad: champion, IT contact, learning lead; propose dates for 2 workshops.
  - Legal/procurement: push a standard SOW + NDA with a 2–3 week procurement timetable. Offer fast-track contracting template.
- Upsell play: Add 3-month adoption support & success measurement add-on.

B. Score 70–84 (Overall 7–8 / “Qualified — Run Pilot & Prove Value”)
- Diagnosis: Good fit; some approvals or technical details to iron out.
- Actions (within 1 week):
  - Propose a Team Workshop Day (£8,800) as a pilot for a high-impact team (e.g., core marketing squad).
  - Offer Executive Briefing (£2,000) to secure executive alignment prior to pilot.
  - Provide case studies from similar clients (adidas, Nestlé) and a sample 30-day KPI dashboard.
  - Agree a simple success metric (e.g., X hours saved across Pilot cohort) and a KPI review date at 30 days.
  - Get procurement/legal pre-reads to confirm sandboxing/permissions.
- Nurture plan: If pilot metrics achieved, propose Complete Sprint and enterprise roll-out.

C. Score 50–69 (Overall 5–6 / “Prospect — Nurture & Educate”)
- Diagnosis: Fit possible but obstacles (budget, timeline, exec buy-in).
- Actions:
  - Recommend Executive Briefing (£2,000) as low-friction first step to build the business case.
  - Offer a tailored ROI model at no cost (limited scope) to help internal champion sell-up.
  - Send a 6–8 week nurture plan: monthly executive insights, 30-minute case-study demos, 1-page ROI templates.
  - Ask for a follow-up decision review date and checklist of procurement/legal blockers to resolve.
- Gate: If after Executive Briefing there is still no internal budget or sponsor, move to long-term nurture.

D. Score 30–49 (Overall 3–4 / “Low Probability — Conditional Nurture”)
- Diagnosis: Either company size marginal, or fundamental blockers (procurement delays, legal bans).
- Actions:
  - Offer to be a resource: send on-demand resources — recorded Executive Briefing, ROI templates, short playbooks.
  - Keep in a 6–12 month nurture track; check back when budget cycles open or when legal constraints change.
  - Capture key changes needed to re-qualify (budget approval, new sponsor, decision date).
- Close politely if no movement after two quarters; move to marketing nurture.

E. Score 1–29 (Overall 1–2 / “Disqualify”)
- Diagnosis: Clear disqualifiers present (legal ban, <50 employees, no budget & no intent).
- Actions:
  - Disqualify from active pipeline but offer resources: invite to newsletter, send recorded briefing & case studies.
  - Log reason for disqualification and next review date (e.g., 6–12 months).
  - Optional: if company is small but influential, propose community/free webinar entry points.

5) Practical guidance for sellers (scripts, timelines & attachments)
- Discovery call length: 30–45 mins with the economic buyer + champion. If only a champion, book for 45 mins and aim to get decision-maker on the Exec Briefing.
- Pre-reads to request: org chart for marketing & product teams, top 3 KPIs they want to impact, existing AI/tooling inventory.
- Suggested follow-up sequence on a strong lead (score >70):
  1) Send tailored proposal + SOW template within 48 hours.
  2) Book Exec Briefing within 7–14 days.
  3) Post-briefing: Pilot Workshop within 14–30 days.
  4) KPI review at 30 days; conversion to Complete Sprint within 60–90 days.
- Contract acceleration tactics: include a fixed-scope SOW for the Complete Sprint, demo of measurable outcomes from a similar client, two reference calls within 48 hrs, and optional payment milestones.
- Template asks for champion: “Can you introduce me to the person who will run day-to-day adoption and sign off on attendance lists?” Use this to convert Authority score.

6) One-page win criteria (use this in proposals & exec briefings)
- Commitment: Executive sponsor identified + budget approval to run pilot or sprint.
- People: Minimum cohort of 20 people for Team Workshop or defined list of 50–150 for enterprise Sprint.
- Tech: Sandbox or safe tooling environment for hands-on labs.
- Metrics: Agreed KPI (e.g., 20+ hours saved per participant / X% reduction in time-to-market).
- Timeline: Pilot or Exec Briefing scheduled within 30 days; pilot KPI review at 30 days.

Wrap-up
- Use the 10 questions as the backbone of your first two calls; score live and decide path (Close/Pilot/Nurture/Disqualify).  
- Keep the conversation outcome-focused (tie to the 90% weekly use & 20+ hrs saved claims) and always seek a named champion + a decision timescale.  
If you want, I can convert the scoring template into a one-click spreadsheet you can use during calls and a short email template per next-step outcome (high/mid/low). Which would you like next?
