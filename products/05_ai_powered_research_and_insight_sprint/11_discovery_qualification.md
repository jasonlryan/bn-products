# AI-Powered Research and Insight Sprint • Discovery Qualification

Below is a practical, sales-ready discovery and qualification framework tailored to Brilliant Noise’s AI‑Powered Research and Insight Sprint. It’s designed for CMOs, CDOs, Innovation Directors and other senior marketing/strategy buyers who need research-backed campaign decisions in days, not weeks.

Executive summary
- Purpose: Help reps rapidly qualify/invalidate opportunities and move deals through an appropriate motion (close, pilot, nurture, or disqualify) for a £10k, 5‑day AI research sprint.
- Focus: Speed-to-insight, competitive advantage, fit with marketing teams under time pressure, and buyer appetite for AI-enabled, expert‑interpreted outputs.

SECTION 1 — 10 discovery questions (organized by BANT + MEDDIC)
Each question is labeled with the primary BANT and MEDDIC dimension it probes. After each question: 1‑line “Why it matters” and a succinct “Strong answer” indicator.

1) “What specific decision are you trying to make in the next 4–8 weeks, and what would a successful outcome look like?”  
   - BANT: Timing | MEDDIC: Metrics / Identify Pain  
   - Why: Confirms concrete campaign/product decision and measurable success criteria.  
   - Strong answer: A defined campaign or launch with KPIs (CTR, conversion lift, positioning, audience segments) and a 2–8 week decision window.

2) “What research have you already got, and where are the gaps or uncertainties that are preventing you from launching confidently?”  
   - BANT: Need | MEDDIC: Identify Pain / Decision Criteria  
   - Why: Reveals pain points (speed, blind spots, competitor signals) the Sprint solves.  
   - Strong answer: Existing quick analytics or gut-based decisions but no multi-source validation, or missing competitor/consumer angle.

3) “Who will sign off on the research and recommendations, and what is their timeline and approval process?”  
   - BANT: Authority | MEDDIC: Economic Buyer / Decision Process  
   - Why: Identifies the decision maker(s) and whether they can move fast.  
   - Strong answer: CMO or Head of Growth with decision authority and a known, short approval path (1–2 stakeholders).

4) “What budget has been allocated (or can be allocated) for short, high‑impact research/insight work this quarter?”  
   - BANT: Budget | MEDDIC: Economic Buyer  
   - Why: Quickly tests affordability for a £10k Sprint vs expectations.  
   - Strong answer: Budget line or discretionary £10k–£20k available; willingness to justify ROI vs £30k+ traditional studies.

5) “How fast do you need validated consumer insight to start the campaign—days, weeks, or months?”  
   - BANT: Timing | MEDDIC: Metrics / Identify Pain  
   - Why: Matches Sprint’s 5‑day promise to buyer urgency.  
   - Strong answer: Need within 1–3 weeks or planning to launch in 4–8 weeks.

6) “How do you currently combine different data sources (social, search, owned analytics, panel) for insight, and what tools/people own that?”  
   - BANT: Need | MEDDIC: Decision Criteria / Champion  
   - Why: Reveals data maturity, integration gaps, and potential internal champions.  
   - Strong answer: Fragmented inputs, manual synthesis, and a marketer or insight lead keen to centralize and accelerate insight.

7) “What outcomes would make you consider a quick external sprint worth the investment—faster go/no-go, creative direction, or measurable forecast lift?”  
   - BANT: Need/Budget | MEDDIC: Metrics / Decision Criteria  
   - Why: Clarifies outcome preferences and ability to measure Sprint impact.  
   - Strong answer: Accepts outcomes like prioritized audience, hypothesis for creative testing, and projected lift metrics to inform media spend.

8) “How competitive is the category right now—are you losing share, seeing fast entrants, or under pressure to respond?”  
   - BANT: Need | MEDDIC: Identify Pain / Metrics  
   - Why: High competitive intensity increases the value of speed and blind‑spot detection.  
   - Strong answer: Rapid launches by competitors, frequent creative churn, or market shifts requiring quick insight.

9) “Who inside your organisation would use the custom dashboard and insights output day‑to‑day, and how would they embed those findings into campaign planning?”  
   - BANT: Authority/Need | MEDDIC: Champion / Decision Criteria  
   - Why: Tests adoption likelihood and identifies internal users/champions.  
   - Strong answer: Heads of Marketing, Media, Brand or Performance with clear processes for campaign brief updates and KPI tracking.

10) “Have you used AI-driven research or external insight sprints before? If so, what worked and what didn’t?”  
   - BANT: Need | MEDDIC: Decision Criteria / Champion  
   - Why: Assesses expectation management, openness to AI, and opportunity to position human QC + expertise.  
   - Strong answer: Curious/positive about AI outputs but disappointed by raw, uncontextualised results—presents an opening to emphasise Brilliant Noise’s expert interpretation.

SECTION 2 — Red flag indicators for disqualification (actionable + product-specific)
If any of these are true, deprioritise or disqualify unless mitigations exist:

- No budget or unwilling to allocate ~£10k for rapid research (explicit “no budget this quarter”).
  - Action: Disqualify or move to long‑term nurture.
- Timing mismatch: decision/launch more than 12 weeks away and no short-term needs.
  - Action: Nurture and revisit closer to planning cycle.
- No clear decision-maker or long, committee-driven procurement (>6 approvers / multiple rounds of procurement).
  - Action: Disqualify or escalate only if champion can consolidate decision authority.
- Buyer insists on traditional 4–6 week methodologies and refuses compressed timelines or AI assistance.
  - Action: Disqualify (product mismatch).
- Data restrictions: regulatory or legal constraints preventing access to required public/social/search analytics or inability to share even high‑level briefs.
  - Action: Disqualify or require legal escalation; could be salvageable if resolved quickly.
- No internal capacity to act on outputs (no campaign owner, no immediate campaign use-case).
  - Action: Disqualify or set up a strategic briefing and longer-term nurture.
- Buyer expects bespoke full‑scale market research (quant-representative panels, tens of thousands sample) that the £10k Sprint can’t deliver.
  - Action: Clarify scope; if insistence persists, disqualify.
- Hostile to AI or vendors offering rapid/interpretive outputs (culture misfit).
  - Action: Disqualify; consider education track only.

SECTION 3 — Ideal customer scoring criteria (1–10 scale) with scoring guidance
Purpose: produce a single 1–10 qualification score to guide next steps. Score each dimension 1–10, calculate weighted average, and map to 1–10 scale.

Recommended dimensions (weights sum to 10). For each, score 1 (poor fit) / 5 (average) / 10 (ideal) with examples.

- Urgency / Time-to-decision (weight 2)  
  - 1 = decision >12 weeks; 5 = decision 6–12 weeks; 10 = decision within 1–4 weeks.

- Budget availability (weight 2)  
  - 1 = no budget; 5 = budget unclear / needs approval; 10 = allocated or discretionary £10k–£20k.

- Decision authority & speed (weight 1.5)  
  - 1 = committee with many approvers; 5 = several stakeholders; 10 = CMO/CDO or single approver able to sign quickly.

- Data-driven maturity & ability to act on insight (weight 1.5)  
  - 1 = no analytics/insight function; 5 = some in-house analytics; 10 = exists performance team that will operationalize outputs.

- Competitive intensity / value of speed (weight 1.5)  
  - 1 = slow-moving category; 5 = moderate churn; 10 = fast launches, many entrants, high need for first-mover insights.

- Fit to target buyer profile (marketing/strategy team, enterprise/global) (weight 1)  
  - 1 = outside marketing remit or non-target industry; 5 = mid-market marketing team; 10 = Global CMO / brand in target industry (FMCG/CPG/tech) with past agency engagement.

- Openness to AI + innovation readiness (weight 0.5)  
  - 1 = anti-AI; 5 = neutral; 10 = proactive AI adoption, previous pilots.

Total weighting = 10.

How to compute:
- Score each dimension 1–10 (use rubric above).  
- Multiply score by its weight, sum weighted scores, divide by total weight (10) to produce a final score 1–10.

Example quick scoring:
- Urgency 9 (×2=18), Budget 8 (×2=16), Authority 7 (×1.5=10.5), Maturity 6 (×1.5=9), Competitive intensity 9 (×1.5=13.5), Buyer fit 8 (×1=8), AI openness 8 (×0.5=4) → weighted sum = 79, divide by 10 = 7.9 → rounded = 8.

Score interpretation and thresholds:
- 8–10 = High fit: Proceed to pilot/proposal fast-track.  
- 6–7 = Moderate fit: Needs short nurture, tailored proposal, involve case studies and commercial flexibility.  
- 4–5 = Low fit: Education + smaller proof-of-value or internal prep work required.  
- 1–3 = Disqualify or long-term nurture (not a fit right now).

SECTION 4 — Next steps and sales playbook by qualification score
Provide concrete steps, collateral, stakeholders to involve, and timelines.

A) Score 8–10 — Close/Accelerate (fast path)
- Objective: Move to contract and schedule Sprint within 1–2 weeks.
- Actions:
  - Immediate win: Send a tailored one-page Sprint scope + deliverables and a proposed 5‑day timeline and statement of work (SOW) with dates and price (£10,000) within 24 hours.
  - Arrange a 30-minute internal stakeholder alignment call including the economic buyer and identified campaign owner to lock scope and sign-off criteria.
  - Secure PO or T&Cs and send client onboarding pack (brief template, data access checklist, stakeholder RACI).
  - Prep team: designate Sprint lead, analyst, and strategist; plan pre-read and kickoff agenda.
- Collateral: Case studies (adidas/Nestlé), one-page ROI model (time-to-launch / cost compare vs traditional research), sample dashboard screenshot.
- Timeline: Contract + kickoff within 7 days; Sprint delivered in 5 business days.

B) Score 6–7 — Pilot / Proof of Value
- Objective: Convert via low-friction pilot or adjusted commercial terms.
- Actions:
  - Propose a tightly scoped pilot Sprint focused on one campaign/market segment or a reduced deliverable set (e.g., focused competitor blind-spot + 2 campaign hypotheses).
  - Offer pilot benefits: success metrics, clear acceptance criteria, and a discount or staged payment if necessary.
  - Schedule a 45‑minute technical/briefing call with the insight lead to confirm data sources and integration needs.
  - Provide tailored collateral: relevant vertical case studies, side-by-side timeline and deliverable clarity.
- Collateral: Short demo of dashboard and 2–3 insight examples; proof of quality controls (human expert review process).
- Timeline: Pilot proposal and kickoff within 2–3 weeks.

C) Score 4–5 — Educate + Build Readiness
- Objective: Nurture, educate on value of rapid AI research, and build internal readiness.
- Actions:
  - Offer a free 20–30 minute strategic consultation to show quick wins (mini-audit) and outline how Sprint could integrate with their campaign calendar.
  - Send tailored content: “How to use Sprint outputs in creative briefs” and an internal adoption checklist for campaign teams.
  - Identify and develop an internal champion; invite them to a webinar or workshop on AI-driven insight adoption.
  - Reassess budget/timing each quarter.
- Collateral: Whitepaper on Sprint methodology, case studies focusing on adoption and ROI, sample brief templates.
- Timeline: Revisit 4–8 weeks or align to their campaign cycles.

D) Score 1–3 — Disqualify or Long‑term Nurture
- Objective: Avoid wasting sales cycles; place them into a nurture program.
- Actions:
  - Politely close the active opportunity with a clear reason and offer to reconnect when conditions change (budget, timing, or campaign urgency).  
  - Add to nurture track: quarterly AI-insight newsletter and targeted case studies; check-in aligned to their planning windows (e.g., pre-season planning).
  - If disqualifying due to policy/data constraints, capture specifics for future re-evaluation.
- Collateral: High-level capability brochure, invite to public webinars.
- Timeline: Recontact aligned to their buying cycle (usually 3–6 months).

SECTION 5 — Rep playbook extras (practical talking points & objection handling)
- Quick value script (close): “For £10k we’ll deliver campaign‑winning insights in 5 days—validated across social, search and your owned analytics—so you can start your next campaign with bulletproof consumer backing instead of gut or a 4–6 week study.”  
- Price objection: “Compared to a £30k traditional study, this gives you 80% faster, actionable insights for campaign decisions—if you need additional scale, we can scope a two‑stage approach.”  
- AI skepticism: “Our AI does the heavy synthesis across signals; every output is reviewed and contextualised by a human strategist so you get accuracy plus interpretation you can act on.”  
- Procurement friction: Offer a standard SOW, a concise vendor onboarding pack, and a one‑page risk/RTO (data & IP) note to limit procurement cycles.

SECTION 6 — Quick checklist for meeting follow-up (operational)
- Send tailored SOW + 5‑day timeline within 24 hours for high-fit prospects.  
- Attach 2–3 most relevant vertical case studies and a sample dashboard export.  
- Request named stakeholders and their availability for a 30-min kickoff within 5 business days after contract.  
- Log scores and red flags in CRM and tag opportunity with recommended next step (Close / Pilot / Nurture / Disqualify).

Closing note
This framework maps directly to Brilliant Noise’s positioning—speed, competitive edge, and expert-interpreted AI insights—and gives field teams a pragmatic, repeatable method to qualify and close Sprint deals quickly. If you want, I can convert the scoring sheet into a one‑page CRM form and provide email templates for each next-step scenario. Which would you prefer next?
